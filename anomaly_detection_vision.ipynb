{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import timm\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/fds/Dev/Python/pytorch/dacon/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_png = sorted(glob(path + 'open/train/*.png'))\n",
    "test_png = sorted(glob(path + 'open/test/*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4277, 2154)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_png), len(test_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_csv(path +\"open/train_df.csv\")\n",
    "\n",
    "train_labels = train_y[\"label\"]\n",
    "\n",
    "label_unique = sorted(np.unique(train_labels))\n",
    "label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n",
    "\n",
    "train_labels = [label_unique[k] for k in train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_load(path):\n",
    "    img = cv2.imread(path)[:,:,::-1]\n",
    "    img = cv2.resize(img, (384, 384),interpolation = cv2.INTER_AREA)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4277/4277 [02:12<00:00, 32.26it/s]\n",
      "100%|██████████| 2154/2154 [01:04<00:00, 33.36it/s]\n"
     ]
    }
   ],
   "source": [
    "train_imgs = [img_load(m) for m in tqdm(train_png)]\n",
    "test_imgs = [img_load(n) for n in tqdm(test_png)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(path + 'train_imgs_384', np.array(train_imgs))\n",
    "np.save(path + 'test_imgs_384', np.array(test_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = np.load(path + 'train_imgs_384.npy')\n",
    "test_imgs = np.load(path + 'test_imgs_384.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 평균 0.4330380901867049 0.4034575319032911 0.39415050509784405\n",
      "train 표준편차 0.1815717110252788 0.17403455556798705 0.16323395055036488\n"
     ]
    }
   ],
   "source": [
    "meanRGB = [np.mean(x, axis=(0,1)) for x in train_imgs]\n",
    "stdRGB = [np.std(x, axis=(0,1)) for x in train_imgs]\n",
    "\n",
    "meanR = np.mean([m[0] for m in meanRGB])/255\n",
    "meanG = np.mean([m[1] for m in meanRGB])/255\n",
    "meanB = np.mean([m[2] for m in meanRGB])/255\n",
    "\n",
    "stdR = np.mean([s[0] for s in stdRGB])/255\n",
    "stdG = np.mean([s[1] for s in stdRGB])/255\n",
    "stdB = np.mean([s[2] for s in stdRGB])/255\n",
    "\n",
    "print(\"train 평균\",meanR, meanG, meanB)\n",
    "print(\"train 표준편차\",stdR, stdG, stdB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_dataset(Dataset):\n",
    "    def __init__(self, img_paths, labels, mode='train'):\n",
    "        self.img_paths = img_paths\n",
    "        self.labels = labels\n",
    "        self.mode=mode\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.img_paths[idx]\n",
    "        if self.mode == 'train':\n",
    "          train_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean = [0.433038, 0.403458, 0.394151],\n",
    "                                     std = [0.181572, 0.174035, 0.163234]),\n",
    "                transforms.RandomAffine((-45, 45)),\n",
    "                \n",
    "            ])\n",
    "          img = train_transform(img)\n",
    "        if self.mode == 'test':\n",
    "          test_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean = [0.418256, 0.393101, 0.386632],\n",
    "                                     std = [0.195055, 0.190053, 0.185323])\n",
    "            ])\n",
    "          img = test_transform(img)\n",
    "\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "    \n",
    "class Network(nn.Module):\n",
    "    def __init__(self,mode = 'train'):\n",
    "        super(Network, self).__init__()\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train':\n",
    "          self.model = timm.create_model('efficientnet_b3', pretrained=True, num_classes=88, drop_path_rate = 0.2)\n",
    "        if self.mode == 'test':\n",
    "          self.model = timm.create_model('efficientnet_b3', pretrained=True, num_classes=88, drop_path_rate = 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function(real, pred):\n",
    "    score = f1_score(real, pred, average=\"macro\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(seed = 2022):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "main(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------fold_0 start!----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra2-cf984f9c.pth\" to /home/fds/.cache/torch/hub/checkpoints/efficientnet_b3_ra2-cf984f9c.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------SAVE:1 epoch----------------\n",
      "epoch : 1/70    time : 174s/12002s\n",
      "TRAIN    loss : 1.18218    f1 : 0.18602\n",
      "Val    loss : 0.62353    f1 : 0.25515\n",
      "-----------------SAVE:2 epoch----------------\n",
      "epoch : 2/70    time : 172s/11678s\n",
      "TRAIN    loss : 0.55778    f1 : 0.36274\n",
      "Val    loss : 0.46246    f1 : 0.39896\n",
      "-----------------SAVE:3 epoch----------------\n",
      "epoch : 3/70    time : 172s/11555s\n",
      "TRAIN    loss : 0.39061    f1 : 0.50992\n",
      "Val    loss : 0.33359    f1 : 0.54776\n",
      "-----------------SAVE:4 epoch----------------\n",
      "epoch : 4/70    time : 175s/11517s\n",
      "TRAIN    loss : 0.28321    f1 : 0.63629\n",
      "Val    loss : 0.34932    f1 : 0.59115\n",
      "-----------------SAVE:5 epoch----------------\n",
      "epoch : 5/70    time : 177s/11473s\n",
      "TRAIN    loss : 0.24074    f1 : 0.68534\n",
      "Val    loss : 0.22370    f1 : 0.63828\n",
      "-----------------SAVE:6 epoch----------------\n",
      "epoch : 6/70    time : 174s/11157s\n",
      "TRAIN    loss : 0.16253    f1 : 0.79660\n",
      "Val    loss : 0.19960    f1 : 0.69963\n",
      "epoch : 7/70    time : 174s/10969s\n",
      "TRAIN    loss : 0.12256    f1 : 0.85105\n",
      "Val    loss : 0.18775    f1 : 0.69887\n",
      "-----------------SAVE:8 epoch----------------\n",
      "epoch : 8/70    time : 176s/10891s\n",
      "TRAIN    loss : 0.09842    f1 : 0.87646\n",
      "Val    loss : 0.19491    f1 : 0.70685\n",
      "-----------------SAVE:9 epoch----------------\n",
      "epoch : 9/70    time : 175s/10656s\n",
      "TRAIN    loss : 0.08432    f1 : 0.90177\n",
      "Val    loss : 0.14709    f1 : 0.76658\n",
      "-----------------SAVE:10 epoch----------------\n",
      "epoch : 10/70    time : 174s/10469s\n",
      "TRAIN    loss : 0.07454    f1 : 0.91639\n",
      "Val    loss : 0.16069    f1 : 0.78869\n",
      "epoch : 11/70    time : 174s/10250s\n",
      "TRAIN    loss : 0.07002    f1 : 0.91966\n",
      "Val    loss : 0.19968    f1 : 0.77316\n",
      "-----------------SAVE:12 epoch----------------\n",
      "epoch : 12/70    time : 175s/10153s\n",
      "TRAIN    loss : 0.04759    f1 : 0.95580\n",
      "Val    loss : 0.15462    f1 : 0.81476\n",
      "epoch : 13/70    time : 174s/9927s\n",
      "TRAIN    loss : 0.04058    f1 : 0.95140\n",
      "Val    loss : 0.20068    f1 : 0.78506\n",
      "epoch : 14/70    time : 174s/9765s\n",
      "TRAIN    loss : 0.04833    f1 : 0.93885\n",
      "Val    loss : 0.18651    f1 : 0.78797\n",
      "-----------------SAVE:15 epoch----------------\n",
      "epoch : 15/70    time : 175s/9614s\n",
      "TRAIN    loss : 0.03513    f1 : 0.96383\n",
      "Val    loss : 0.15325    f1 : 0.83644\n",
      "epoch : 16/70    time : 174s/9419s\n",
      "TRAIN    loss : 0.03431    f1 : 0.96930\n",
      "Val    loss : 0.18535    f1 : 0.78548\n",
      "epoch : 17/70    time : 174s/9242s\n",
      "TRAIN    loss : 0.03140    f1 : 0.96390\n",
      "Val    loss : 0.19372    f1 : 0.78457\n",
      "epoch : 18/70    time : 174s/9052s\n",
      "TRAIN    loss : 0.02559    f1 : 0.98066\n",
      "Val    loss : 0.15691    f1 : 0.83201\n",
      "epoch : 19/70    time : 174s/8881s\n",
      "TRAIN    loss : 0.02854    f1 : 0.98232\n",
      "Val    loss : 0.17053    f1 : 0.79920\n",
      "epoch : 20/70    time : 174s/8704s\n",
      "TRAIN    loss : 0.03655    f1 : 0.96462\n",
      "Val    loss : 0.19092    f1 : 0.77429\n",
      "epoch : 21/70    time : 174s/8527s\n",
      "TRAIN    loss : 0.02970    f1 : 0.97410\n",
      "Val    loss : 0.15785    f1 : 0.82145\n",
      "epoch : 22/70    time : 174s/8368s\n",
      "TRAIN    loss : 0.03267    f1 : 0.96300\n",
      "Val    loss : 0.18227    f1 : 0.79631\n",
      "epoch : 23/70    time : 174s/8195s\n",
      "TRAIN    loss : 0.01426    f1 : 0.98433\n",
      "Val    loss : 0.17179    f1 : 0.81279\n",
      "epoch : 24/70    time : 174s/8012s\n",
      "TRAIN    loss : 0.02786    f1 : 0.97234\n",
      "Val    loss : 0.18970    f1 : 0.78739\n",
      "epoch : 25/70    time : 174s/7842s\n",
      "TRAIN    loss : 0.02497    f1 : 0.98093\n",
      "Val    loss : 0.19650    f1 : 0.82420\n",
      "epoch : 26/70    time : 174s/7663s\n",
      "TRAIN    loss : 0.02212    f1 : 0.98600\n",
      "Val    loss : 0.19555    f1 : 0.79996\n",
      "epoch : 27/70    time : 174s/7490s\n",
      "TRAIN    loss : 0.02390    f1 : 0.97609\n",
      "Val    loss : 0.17670    f1 : 0.82175\n",
      "epoch : 28/70    time : 174s/7315s\n",
      "TRAIN    loss : 0.02393    f1 : 0.97983\n",
      "Val    loss : 0.17629    f1 : 0.81664\n",
      "epoch : 29/70    time : 174s/7129s\n",
      "TRAIN    loss : 0.04153    f1 : 0.95590\n",
      "Val    loss : 0.17603    f1 : 0.79709\n",
      "epoch : 30/70    time : 174s/6958s\n",
      "TRAIN    loss : 0.03460    f1 : 0.96468\n",
      "Val    loss : 0.19029    f1 : 0.79611\n",
      "epoch : 31/70    time : 175s/6833s\n",
      "TRAIN    loss : 0.01793    f1 : 0.98685\n",
      "Val    loss : 0.16776    f1 : 0.78097\n",
      "epoch : 32/70    time : 175s/6645s\n",
      "TRAIN    loss : 0.01611    f1 : 0.98310\n",
      "Val    loss : 0.15552    f1 : 0.80666\n",
      "epoch : 33/70    time : 175s/6473s\n",
      "TRAIN    loss : 0.01296    f1 : 0.99030\n",
      "Val    loss : 0.17032    f1 : 0.81614\n",
      "epoch : 34/70    time : 175s/6293s\n",
      "TRAIN    loss : 0.01555    f1 : 0.98717\n",
      "Val    loss : 0.17673    f1 : 0.81662\n",
      "epoch : 35/70    time : 175s/6108s\n",
      "TRAIN    loss : 0.03144    f1 : 0.97949\n",
      "Val    loss : 0.14255    f1 : 0.81358\n",
      "----------fold_1 start!----------\n",
      "-----------------SAVE:1 epoch----------------\n",
      "epoch : 1/70    time : 175s/12054s\n",
      "TRAIN    loss : 1.14439    f1 : 0.18126\n",
      "Val    loss : 0.61657    f1 : 0.28334\n",
      "-----------------SAVE:2 epoch----------------\n",
      "epoch : 2/70    time : 175s/11905s\n",
      "TRAIN    loss : 0.54485    f1 : 0.35460\n",
      "Val    loss : 0.40395    f1 : 0.44280\n",
      "-----------------SAVE:3 epoch----------------\n",
      "epoch : 3/70    time : 175s/11751s\n",
      "TRAIN    loss : 0.36657    f1 : 0.49938\n",
      "Val    loss : 0.30406    f1 : 0.57295\n",
      "-----------------SAVE:4 epoch----------------\n",
      "epoch : 4/70    time : 176s/11585s\n",
      "TRAIN    loss : 0.26966    f1 : 0.65343\n",
      "Val    loss : 0.23489    f1 : 0.63582\n",
      "-----------------SAVE:5 epoch----------------\n",
      "epoch : 5/70    time : 176s/11409s\n",
      "TRAIN    loss : 0.20728    f1 : 0.71056\n",
      "Val    loss : 0.22954    f1 : 0.66394\n",
      "-----------------SAVE:6 epoch----------------\n",
      "epoch : 6/70    time : 176s/11235s\n",
      "TRAIN    loss : 0.16136    f1 : 0.79710\n",
      "Val    loss : 0.20844    f1 : 0.68538\n",
      "-----------------SAVE:7 epoch----------------\n",
      "epoch : 7/70    time : 175s/11050s\n",
      "TRAIN    loss : 0.11681    f1 : 0.84655\n",
      "Val    loss : 0.21073    f1 : 0.72248\n",
      "-----------------SAVE:8 epoch----------------\n",
      "epoch : 8/70    time : 176s/10881s\n",
      "TRAIN    loss : 0.09574    f1 : 0.88415\n",
      "Val    loss : 0.19076    f1 : 0.73480\n",
      "-----------------SAVE:9 epoch----------------\n",
      "epoch : 9/70    time : 176s/10712s\n",
      "TRAIN    loss : 0.09637    f1 : 0.88359\n",
      "Val    loss : 0.14963    f1 : 0.75041\n",
      "-----------------SAVE:10 epoch----------------\n",
      "epoch : 10/70    time : 176s/10586s\n",
      "TRAIN    loss : 0.05856    f1 : 0.92873\n",
      "Val    loss : 0.15452    f1 : 0.80956\n",
      "epoch : 11/70    time : 175s/10309s\n",
      "TRAIN    loss : 0.04986    f1 : 0.94372\n",
      "Val    loss : 0.19084    f1 : 0.79882\n",
      "epoch : 12/70    time : 175s/10162s\n",
      "TRAIN    loss : 0.05026    f1 : 0.94051\n",
      "Val    loss : 0.16339    f1 : 0.79151\n",
      "epoch : 13/70    time : 176s/10011s\n",
      "TRAIN    loss : 0.05253    f1 : 0.95142\n",
      "Val    loss : 0.16511    f1 : 0.79686\n",
      "epoch : 14/70    time : 175s/9797s\n",
      "TRAIN    loss : 0.03824    f1 : 0.95893\n",
      "Val    loss : 0.17140    f1 : 0.78981\n",
      "epoch : 15/70    time : 174s/9545s\n",
      "TRAIN    loss : 0.03583    f1 : 0.96300\n",
      "Val    loss : 0.17695    f1 : 0.77916\n",
      "epoch : 16/70    time : 173s/9323s\n",
      "TRAIN    loss : 0.03843    f1 : 0.95941\n",
      "Val    loss : 0.15228    f1 : 0.78718\n",
      "-----------------SAVE:17 epoch----------------\n",
      "epoch : 17/70    time : 173s/9184s\n",
      "TRAIN    loss : 0.04051    f1 : 0.95545\n",
      "Val    loss : 0.16212    f1 : 0.84627\n",
      "epoch : 18/70    time : 173s/8994s\n",
      "TRAIN    loss : 0.03031    f1 : 0.96119\n",
      "Val    loss : 0.17371    f1 : 0.80944\n",
      "epoch : 19/70    time : 173s/8815s\n",
      "TRAIN    loss : 0.03575    f1 : 0.94936\n",
      "Val    loss : 0.17142    f1 : 0.79552\n",
      "epoch : 20/70    time : 173s/8634s\n",
      "TRAIN    loss : 0.03633    f1 : 0.95107\n",
      "Val    loss : 0.18820    f1 : 0.80242\n",
      "epoch : 21/70    time : 174s/8503s\n",
      "TRAIN    loss : 0.02599    f1 : 0.96777\n",
      "Val    loss : 0.16733    f1 : 0.83443\n",
      "epoch : 22/70    time : 176s/8435s\n",
      "TRAIN    loss : 0.03523    f1 : 0.96360\n",
      "Val    loss : 0.16505    f1 : 0.80985\n",
      "epoch : 23/70    time : 174s/8182s\n",
      "TRAIN    loss : 0.03830    f1 : 0.94692\n",
      "Val    loss : 0.20732    f1 : 0.78916\n",
      "epoch : 24/70    time : 175s/8058s\n",
      "TRAIN    loss : 0.02067    f1 : 0.97572\n",
      "Val    loss : 0.17524    f1 : 0.81988\n",
      "epoch : 25/70    time : 175s/7894s\n",
      "TRAIN    loss : 0.01217    f1 : 0.98553\n",
      "Val    loss : 0.15375    f1 : 0.81978\n",
      "epoch : 26/70    time : 174s/7667s\n",
      "TRAIN    loss : 0.02588    f1 : 0.97146\n",
      "Val    loss : 0.18587    f1 : 0.77460\n",
      "epoch : 27/70    time : 176s/7553s\n",
      "TRAIN    loss : 0.01290    f1 : 0.98965\n",
      "Val    loss : 0.16519    f1 : 0.83528\n",
      "epoch : 28/70    time : 175s/7356s\n",
      "TRAIN    loss : 0.01127    f1 : 0.98970\n",
      "Val    loss : 0.16615    f1 : 0.82604\n",
      "epoch : 29/70    time : 175s/7175s\n",
      "TRAIN    loss : 0.02526    f1 : 0.97676\n",
      "Val    loss : 0.22003    f1 : 0.79704\n",
      "epoch : 30/70    time : 175s/6983s\n",
      "TRAIN    loss : 0.02985    f1 : 0.97616\n",
      "Val    loss : 0.16518    f1 : 0.80050\n",
      "epoch : 31/70    time : 174s/6794s\n",
      "TRAIN    loss : 0.03473    f1 : 0.96125\n",
      "Val    loss : 0.21537    f1 : 0.79104\n",
      "epoch : 32/70    time : 175s/6633s\n",
      "TRAIN    loss : 0.03293    f1 : 0.96897\n",
      "Val    loss : 0.17745    f1 : 0.82018\n",
      "epoch : 33/70    time : 174s/6442s\n",
      "TRAIN    loss : 0.02156    f1 : 0.97157\n",
      "Val    loss : 0.24738    f1 : 0.82438\n",
      "epoch : 34/70    time : 175s/6283s\n",
      "TRAIN    loss : 0.02134    f1 : 0.97135\n",
      "Val    loss : 0.16614    f1 : 0.80981\n",
      "epoch : 35/70    time : 175s/6116s\n",
      "TRAIN    loss : 0.01626    f1 : 0.97817\n",
      "Val    loss : 0.19227    f1 : 0.81106\n",
      "-----------------SAVE:36 epoch----------------\n",
      "epoch : 36/70    time : 175s/5950s\n",
      "TRAIN    loss : 0.00947    f1 : 0.99289\n",
      "Val    loss : 0.17510    f1 : 0.85411\n",
      "-----------------SAVE:37 epoch----------------\n",
      "epoch : 37/70    time : 175s/5767s\n",
      "TRAIN    loss : 0.00331    f1 : 0.99761\n",
      "Val    loss : 0.16822    f1 : 0.85752\n",
      "epoch : 38/70    time : 174s/5577s\n",
      "TRAIN    loss : 0.01016    f1 : 0.99183\n",
      "Val    loss : 0.18476    f1 : 0.82844\n",
      "epoch : 39/70    time : 174s/5398s\n",
      "TRAIN    loss : 0.01483    f1 : 0.98889\n",
      "Val    loss : 0.22221    f1 : 0.81701\n",
      "epoch : 40/70    time : 174s/5232s\n",
      "TRAIN    loss : 0.02250    f1 : 0.97873\n",
      "Val    loss : 0.22658    f1 : 0.77640\n",
      "epoch : 41/70    time : 175s/5062s\n",
      "TRAIN    loss : 0.01611    f1 : 0.98204\n",
      "Val    loss : 0.18826    f1 : 0.83522\n",
      "epoch : 42/70    time : 174s/4885s\n",
      "TRAIN    loss : 0.02035    f1 : 0.98172\n",
      "Val    loss : 0.22536    f1 : 0.81949\n",
      "epoch : 43/70    time : 174s/4710s\n",
      "TRAIN    loss : 0.01792    f1 : 0.97893\n",
      "Val    loss : 0.18372    f1 : 0.82235\n",
      "epoch : 44/70    time : 174s/4526s\n",
      "TRAIN    loss : 0.02371    f1 : 0.97596\n",
      "Val    loss : 0.16681    f1 : 0.83194\n",
      "epoch : 45/70    time : 177s/4423s\n",
      "TRAIN    loss : 0.02161    f1 : 0.98463\n",
      "Val    loss : 0.15711    f1 : 0.79838\n",
      "epoch : 46/70    time : 178s/4279s\n",
      "TRAIN    loss : 0.02772    f1 : 0.97594\n",
      "Val    loss : 0.14025    f1 : 0.84554\n",
      "-----------------SAVE:47 epoch----------------\n",
      "epoch : 47/70    time : 177s/4064s\n",
      "TRAIN    loss : 0.01486    f1 : 0.98654\n",
      "Val    loss : 0.13300    f1 : 0.86919\n",
      "-----------------SAVE:48 epoch----------------\n",
      "epoch : 48/70    time : 177s/3900s\n",
      "TRAIN    loss : 0.01772    f1 : 0.99016\n",
      "Val    loss : 0.11645    f1 : 0.87718\n",
      "epoch : 49/70    time : 176s/3688s\n",
      "TRAIN    loss : 0.01582    f1 : 0.97844\n",
      "Val    loss : 0.14955    f1 : 0.84213\n",
      "epoch : 50/70    time : 176s/3523s\n",
      "TRAIN    loss : 0.01193    f1 : 0.98046\n",
      "Val    loss : 0.11407    f1 : 0.86049\n",
      "epoch : 51/70    time : 176s/3338s\n",
      "TRAIN    loss : 0.01673    f1 : 0.98872\n",
      "Val    loss : 0.12045    f1 : 0.87102\n",
      "epoch : 52/70    time : 175s/3155s\n",
      "TRAIN    loss : 0.01163    f1 : 0.98677\n",
      "Val    loss : 0.15837    f1 : 0.82437\n",
      "epoch : 53/70    time : 175s/2977s\n",
      "TRAIN    loss : 0.00707    f1 : 0.99545\n",
      "Val    loss : 0.14251    f1 : 0.84663\n",
      "epoch : 54/70    time : 175s/2796s\n",
      "TRAIN    loss : 0.01407    f1 : 0.98602\n",
      "Val    loss : 0.18248    f1 : 0.80366\n",
      "epoch : 55/70    time : 175s/2622s\n",
      "TRAIN    loss : 0.01403    f1 : 0.98697\n",
      "Val    loss : 0.19731    f1 : 0.81830\n",
      "epoch : 56/70    time : 175s/2451s\n",
      "TRAIN    loss : 0.00704    f1 : 0.99432\n",
      "Val    loss : 0.14193    f1 : 0.86055\n",
      "epoch : 57/70    time : 175s/2276s\n",
      "TRAIN    loss : 0.00694    f1 : 0.99335\n",
      "Val    loss : 0.12781    f1 : 0.84415\n",
      "epoch : 58/70    time : 175s/2097s\n",
      "TRAIN    loss : 0.01018    f1 : 0.98536\n",
      "Val    loss : 0.14933    f1 : 0.82541\n",
      "epoch : 59/70    time : 174s/1919s\n",
      "TRAIN    loss : 0.01722    f1 : 0.98350\n",
      "Val    loss : 0.18506    f1 : 0.82383\n",
      "epoch : 60/70    time : 174s/1744s\n",
      "TRAIN    loss : 0.01191    f1 : 0.99160\n",
      "Val    loss : 0.19721    f1 : 0.82619\n",
      "epoch : 61/70    time : 175s/1572s\n",
      "TRAIN    loss : 0.01122    f1 : 0.99005\n",
      "Val    loss : 0.16966    f1 : 0.83631\n",
      "epoch : 62/70    time : 174s/1395s\n",
      "TRAIN    loss : 0.00550    f1 : 0.99625\n",
      "Val    loss : 0.14835    f1 : 0.86209\n",
      "-----------------SAVE:63 epoch----------------\n",
      "epoch : 63/70    time : 175s/1223s\n",
      "TRAIN    loss : 0.00795    f1 : 0.99773\n",
      "Val    loss : 0.11601    f1 : 0.88554\n",
      "epoch : 64/70    time : 175s/1049s\n",
      "TRAIN    loss : 0.01074    f1 : 0.98946\n",
      "Val    loss : 0.15524    f1 : 0.84454\n",
      "epoch : 65/70    time : 176s/879s\n",
      "TRAIN    loss : 0.01405    f1 : 0.99045\n",
      "Val    loss : 0.14140    f1 : 0.85364\n",
      "epoch : 66/70    time : 177s/710s\n",
      "TRAIN    loss : 0.01002    f1 : 0.99002\n",
      "Val    loss : 0.23666    f1 : 0.79949\n",
      "epoch : 67/70    time : 172s/515s\n",
      "TRAIN    loss : 0.02009    f1 : 0.98203\n",
      "Val    loss : 0.18221    f1 : 0.84545\n",
      "epoch : 68/70    time : 170s/341s\n",
      "TRAIN    loss : 0.01605    f1 : 0.98648\n",
      "Val    loss : 0.17714    f1 : 0.80175\n",
      "epoch : 69/70    time : 170s/170s\n",
      "TRAIN    loss : 0.02155    f1 : 0.98386\n",
      "Val    loss : 0.18116    f1 : 0.82944\n",
      "epoch : 70/70    time : 170s/0s\n",
      "TRAIN    loss : 0.01816    f1 : 0.98131\n",
      "Val    loss : 0.18456    f1 : 0.83401\n",
      "----------fold_2 start!----------\n",
      "-----------------SAVE:1 epoch----------------\n",
      "epoch : 1/70    time : 173s/11939s\n",
      "TRAIN    loss : 1.17597    f1 : 0.16806\n",
      "Val    loss : 0.60226    f1 : 0.26405\n",
      "-----------------SAVE:2 epoch----------------\n",
      "epoch : 2/70    time : 171s/11601s\n",
      "TRAIN    loss : 0.54331    f1 : 0.35012\n",
      "Val    loss : 0.47995    f1 : 0.35555\n",
      "-----------------SAVE:3 epoch----------------\n",
      "epoch : 3/70    time : 175s/11715s\n",
      "TRAIN    loss : 0.41002    f1 : 0.49052\n",
      "Val    loss : 0.31887    f1 : 0.51932\n",
      "-----------------SAVE:4 epoch----------------\n",
      "epoch : 4/70    time : 176s/11630s\n",
      "TRAIN    loss : 0.26349    f1 : 0.63777\n",
      "Val    loss : 0.28457    f1 : 0.54534\n",
      "-----------------SAVE:5 epoch----------------\n",
      "epoch : 5/70    time : 177s/11485s\n",
      "TRAIN    loss : 0.22157    f1 : 0.71092\n",
      "Val    loss : 0.20892    f1 : 0.71859\n",
      "-----------------SAVE:6 epoch----------------\n",
      "epoch : 6/70    time : 176s/11271s\n",
      "TRAIN    loss : 0.13990    f1 : 0.81100\n",
      "Val    loss : 0.18634    f1 : 0.72159\n",
      "-----------------SAVE:7 epoch----------------\n",
      "epoch : 7/70    time : 175s/11020s\n",
      "TRAIN    loss : 0.13386    f1 : 0.85135\n",
      "Val    loss : 0.17500    f1 : 0.75506\n",
      "-----------------SAVE:8 epoch----------------\n",
      "epoch : 8/70    time : 174s/10815s\n",
      "TRAIN    loss : 0.08886    f1 : 0.88369\n",
      "Val    loss : 0.14641    f1 : 0.75962\n",
      "-----------------SAVE:9 epoch----------------\n",
      "epoch : 9/70    time : 178s/10837s\n",
      "TRAIN    loss : 0.08143    f1 : 0.89442\n",
      "Val    loss : 0.13967    f1 : 0.77031\n",
      "epoch : 10/70    time : 176s/10572s\n",
      "TRAIN    loss : 0.06000    f1 : 0.93121\n",
      "Val    loss : 0.15147    f1 : 0.75835\n",
      "epoch : 11/70    time : 177s/10437s\n",
      "TRAIN    loss : 0.06185    f1 : 0.94389\n",
      "Val    loss : 0.16457    f1 : 0.72344\n",
      "epoch : 12/70    time : 178s/10332s\n",
      "TRAIN    loss : 0.04976    f1 : 0.94653\n",
      "Val    loss : 0.16495    f1 : 0.72544\n",
      "epoch : 13/70    time : 176s/10021s\n",
      "TRAIN    loss : 0.05165    f1 : 0.93068\n",
      "Val    loss : 0.19836    f1 : 0.71615\n",
      "epoch : 14/70    time : 175s/9812s\n",
      "TRAIN    loss : 0.03816    f1 : 0.95563\n",
      "Val    loss : 0.21349    f1 : 0.73255\n",
      "epoch : 15/70    time : 175s/9650s\n",
      "TRAIN    loss : 0.02549    f1 : 0.97886\n",
      "Val    loss : 0.16079    f1 : 0.75876\n",
      "epoch : 16/70    time : 174s/9421s\n",
      "TRAIN    loss : 0.03064    f1 : 0.97380\n",
      "Val    loss : 0.17704    f1 : 0.74359\n",
      "epoch : 17/70    time : 175s/9267s\n",
      "TRAIN    loss : 0.03695    f1 : 0.96782\n",
      "Val    loss : 0.19275    f1 : 0.73838\n",
      "epoch : 18/70    time : 174s/9064s\n",
      "TRAIN    loss : 0.03873    f1 : 0.95044\n",
      "Val    loss : 0.19952    f1 : 0.72990\n",
      "epoch : 19/70    time : 175s/8911s\n",
      "TRAIN    loss : 0.02459    f1 : 0.97534\n",
      "Val    loss : 0.15754    f1 : 0.76840\n",
      "epoch : 20/70    time : 171s/8546s\n",
      "TRAIN    loss : 0.02584    f1 : 0.97031\n",
      "Val    loss : 0.19479    f1 : 0.74412\n",
      "epoch : 21/70    time : 170s/8344s\n",
      "TRAIN    loss : 0.03672    f1 : 0.96210\n",
      "Val    loss : 0.20185    f1 : 0.75002\n",
      "-----------------SAVE:22 epoch----------------\n",
      "epoch : 22/70    time : 171s/8201s\n",
      "TRAIN    loss : 0.02412    f1 : 0.97764\n",
      "Val    loss : 0.16198    f1 : 0.79712\n",
      "epoch : 23/70    time : 170s/8006s\n",
      "TRAIN    loss : 0.02395    f1 : 0.97711\n",
      "Val    loss : 0.16085    f1 : 0.78665\n",
      "epoch : 24/70    time : 170s/7832s\n",
      "TRAIN    loss : 0.01588    f1 : 0.97568\n",
      "Val    loss : 0.18363    f1 : 0.77238\n",
      "epoch : 25/70    time : 171s/7689s\n",
      "TRAIN    loss : 0.01833    f1 : 0.98091\n",
      "Val    loss : 0.20199    f1 : 0.77781\n",
      "epoch : 26/70    time : 175s/7681s\n",
      "TRAIN    loss : 0.02311    f1 : 0.97576\n",
      "Val    loss : 0.18352    f1 : 0.77049\n",
      "epoch : 27/70    time : 174s/7492s\n",
      "TRAIN    loss : 0.02758    f1 : 0.97350\n",
      "Val    loss : 0.17683    f1 : 0.77768\n",
      "-----------------SAVE:28 epoch----------------\n",
      "epoch : 28/70    time : 175s/7361s\n",
      "TRAIN    loss : 0.02688    f1 : 0.97788\n",
      "Val    loss : 0.16254    f1 : 0.80537\n",
      "epoch : 29/70    time : 175s/7156s\n",
      "TRAIN    loss : 0.02555    f1 : 0.97145\n",
      "Val    loss : 0.21966    f1 : 0.71249\n",
      "epoch : 30/70    time : 175s/6983s\n",
      "TRAIN    loss : 0.02197    f1 : 0.97370\n",
      "Val    loss : 0.21655    f1 : 0.71987\n",
      "epoch : 31/70    time : 175s/6826s\n",
      "TRAIN    loss : 0.02618    f1 : 0.98145\n",
      "Val    loss : 0.16390    f1 : 0.73456\n",
      "epoch : 32/70    time : 174s/6627s\n",
      "TRAIN    loss : 0.02464    f1 : 0.98116\n",
      "Val    loss : 0.17449    f1 : 0.78243\n",
      "epoch : 33/70    time : 175s/6470s\n",
      "TRAIN    loss : 0.02437    f1 : 0.97758\n",
      "Val    loss : 0.18188    f1 : 0.75104\n",
      "-----------------SAVE:34 epoch----------------\n",
      "epoch : 34/70    time : 175s/6308s\n",
      "TRAIN    loss : 0.02156    f1 : 0.96800\n",
      "Val    loss : 0.16735    f1 : 0.80553\n",
      "-----------------SAVE:35 epoch----------------\n",
      "epoch : 35/70    time : 175s/6134s\n",
      "TRAIN    loss : 0.01770    f1 : 0.97932\n",
      "Val    loss : 0.13760    f1 : 0.83836\n",
      "epoch : 36/70    time : 175s/5937s\n",
      "TRAIN    loss : 0.01270    f1 : 0.98500\n",
      "Val    loss : 0.19181    f1 : 0.79130\n",
      "epoch : 37/70    time : 174s/5756s\n",
      "TRAIN    loss : 0.01166    f1 : 0.98676\n",
      "Val    loss : 0.23564    f1 : 0.77912\n",
      "epoch : 38/70    time : 175s/5586s\n",
      "TRAIN    loss : 0.02763    f1 : 0.96698\n",
      "Val    loss : 0.15256    f1 : 0.82129\n",
      "epoch : 39/70    time : 175s/5419s\n",
      "TRAIN    loss : 0.02384    f1 : 0.98110\n",
      "Val    loss : 0.17128    f1 : 0.77844\n",
      "epoch : 40/70    time : 175s/5243s\n",
      "TRAIN    loss : 0.02102    f1 : 0.98569\n",
      "Val    loss : 0.20427    f1 : 0.81434\n",
      "epoch : 41/70    time : 175s/5071s\n",
      "TRAIN    loss : 0.02359    f1 : 0.99102\n",
      "Val    loss : 0.18407    f1 : 0.79550\n",
      "epoch : 42/70    time : 175s/4893s\n",
      "TRAIN    loss : 0.01193    f1 : 0.98509\n",
      "Val    loss : 0.19301    f1 : 0.79436\n",
      "epoch : 43/70    time : 175s/4719s\n",
      "TRAIN    loss : 0.00960    f1 : 0.98706\n",
      "Val    loss : 0.18566    f1 : 0.81002\n",
      "epoch : 44/70    time : 175s/4546s\n",
      "TRAIN    loss : 0.02068    f1 : 0.97374\n",
      "Val    loss : 0.20307    f1 : 0.82026\n",
      "epoch : 45/70    time : 174s/4361s\n",
      "TRAIN    loss : 0.01881    f1 : 0.98091\n",
      "Val    loss : 0.16765    f1 : 0.81739\n",
      "epoch : 46/70    time : 175s/4189s\n",
      "TRAIN    loss : 0.00422    f1 : 0.99657\n",
      "Val    loss : 0.17133    f1 : 0.79230\n",
      "epoch : 47/70    time : 175s/4023s\n",
      "TRAIN    loss : 0.03268    f1 : 0.98204\n",
      "Val    loss : 0.15409    f1 : 0.82207\n",
      "epoch : 48/70    time : 175s/3847s\n",
      "TRAIN    loss : 0.03020    f1 : 0.97650\n",
      "Val    loss : 0.14643    f1 : 0.80205\n",
      "epoch : 49/70    time : 175s/3668s\n",
      "TRAIN    loss : 0.01468    f1 : 0.98612\n",
      "Val    loss : 0.18408    f1 : 0.81289\n",
      "epoch : 50/70    time : 174s/3489s\n",
      "TRAIN    loss : 0.01438    f1 : 0.98287\n",
      "Val    loss : 0.14204    f1 : 0.83554\n",
      "epoch : 51/70    time : 175s/3317s\n",
      "TRAIN    loss : 0.00606    f1 : 0.98923\n",
      "Val    loss : 0.14735    f1 : 0.83774\n",
      "epoch : 52/70    time : 175s/3142s\n",
      "TRAIN    loss : 0.00797    f1 : 0.98563\n",
      "Val    loss : 0.16741    f1 : 0.81160\n",
      "epoch : 53/70    time : 174s/2966s\n",
      "TRAIN    loss : 0.00574    f1 : 0.99426\n",
      "Val    loss : 0.17884    f1 : 0.83062\n",
      "epoch : 54/70    time : 175s/2793s\n",
      "TRAIN    loss : 0.00287    f1 : 0.99791\n",
      "Val    loss : 0.17559    f1 : 0.81430\n",
      "epoch : 55/70    time : 174s/2616s\n",
      "TRAIN    loss : 0.00751    f1 : 0.99541\n",
      "Val    loss : 0.16958    f1 : 0.80214\n",
      "----------fold_3 start!----------\n",
      "-----------------SAVE:1 epoch----------------\n",
      "epoch : 1/70    time : 174s/12031s\n",
      "TRAIN    loss : 1.19266    f1 : 0.16875\n",
      "Val    loss : 0.67386    f1 : 0.26199\n",
      "-----------------SAVE:2 epoch----------------\n",
      "epoch : 2/70    time : 175s/11888s\n",
      "TRAIN    loss : 0.57066    f1 : 0.31087\n",
      "Val    loss : 0.46387    f1 : 0.42605\n",
      "-----------------SAVE:3 epoch----------------\n",
      "epoch : 3/70    time : 175s/11712s\n",
      "TRAIN    loss : 0.39200    f1 : 0.50368\n",
      "Val    loss : 0.34034    f1 : 0.50764\n",
      "-----------------SAVE:4 epoch----------------\n",
      "epoch : 4/70    time : 175s/11568s\n",
      "TRAIN    loss : 0.28777    f1 : 0.60443\n",
      "Val    loss : 0.23917    f1 : 0.63288\n",
      "-----------------SAVE:5 epoch----------------\n",
      "epoch : 5/70    time : 175s/11390s\n",
      "TRAIN    loss : 0.21583    f1 : 0.72298\n",
      "Val    loss : 0.25468    f1 : 0.65797\n",
      "-----------------SAVE:6 epoch----------------\n",
      "epoch : 6/70    time : 175s/11204s\n",
      "TRAIN    loss : 0.15063    f1 : 0.80409\n",
      "Val    loss : 0.16455    f1 : 0.74467\n",
      "-----------------SAVE:7 epoch----------------\n",
      "epoch : 7/70    time : 175s/11006s\n",
      "TRAIN    loss : 0.12440    f1 : 0.82229\n",
      "Val    loss : 0.16574    f1 : 0.76586\n",
      "-----------------SAVE:8 epoch----------------\n",
      "epoch : 8/70    time : 175s/10854s\n",
      "TRAIN    loss : 0.09920    f1 : 0.89201\n",
      "Val    loss : 0.13104    f1 : 0.80008\n",
      "epoch : 9/70    time : 174s/10642s\n",
      "TRAIN    loss : 0.07638    f1 : 0.91033\n",
      "Val    loss : 0.14721    f1 : 0.78823\n",
      "epoch : 10/70    time : 174s/10453s\n",
      "TRAIN    loss : 0.07745    f1 : 0.89510\n",
      "Val    loss : 0.17678    f1 : 0.71927\n",
      "epoch : 11/70    time : 174s/10284s\n",
      "TRAIN    loss : 0.06240    f1 : 0.94531\n",
      "Val    loss : 0.13365    f1 : 0.75841\n",
      "-----------------SAVE:12 epoch----------------\n",
      "epoch : 12/70    time : 175s/10166s\n",
      "TRAIN    loss : 0.05350    f1 : 0.94027\n",
      "Val    loss : 0.14462    f1 : 0.80645\n",
      "-----------------SAVE:13 epoch----------------\n",
      "epoch : 13/70    time : 175s/9979s\n",
      "TRAIN    loss : 0.04032    f1 : 0.96558\n",
      "Val    loss : 0.15342    f1 : 0.82351\n",
      "epoch : 14/70    time : 175s/9773s\n",
      "TRAIN    loss : 0.03443    f1 : 0.96562\n",
      "Val    loss : 0.15259    f1 : 0.78952\n",
      "-----------------SAVE:15 epoch----------------\n",
      "epoch : 15/70    time : 175s/9612s\n",
      "TRAIN    loss : 0.03564    f1 : 0.96018\n",
      "Val    loss : 0.15789    f1 : 0.83527\n",
      "epoch : 16/70    time : 174s/9416s\n",
      "TRAIN    loss : 0.03591    f1 : 0.96243\n",
      "Val    loss : 0.13104    f1 : 0.82810\n",
      "epoch : 17/70    time : 174s/9244s\n",
      "TRAIN    loss : 0.02950    f1 : 0.95785\n",
      "Val    loss : 0.17712    f1 : 0.78386\n",
      "epoch : 18/70    time : 174s/9065s\n",
      "TRAIN    loss : 0.04307    f1 : 0.95859\n",
      "Val    loss : 0.18263    f1 : 0.75412\n",
      "epoch : 19/70    time : 175s/8904s\n",
      "TRAIN    loss : 0.04056    f1 : 0.95662\n",
      "Val    loss : 0.13699    f1 : 0.82055\n",
      "epoch : 20/70    time : 175s/8725s\n",
      "TRAIN    loss : 0.01737    f1 : 0.97694\n",
      "Val    loss : 0.14109    f1 : 0.80425\n",
      "epoch : 21/70    time : 175s/8571s\n",
      "TRAIN    loss : 0.02457    f1 : 0.97548\n",
      "Val    loss : 0.13949    f1 : 0.80826\n",
      "epoch : 22/70    time : 174s/8373s\n",
      "TRAIN    loss : 0.01611    f1 : 0.98629\n",
      "Val    loss : 0.12465    f1 : 0.82790\n",
      "epoch : 23/70    time : 175s/8203s\n",
      "TRAIN    loss : 0.01699    f1 : 0.98417\n",
      "Val    loss : 0.16306    f1 : 0.78307\n",
      "epoch : 24/70    time : 174s/8026s\n",
      "TRAIN    loss : 0.02200    f1 : 0.97670\n",
      "Val    loss : 0.15221    f1 : 0.80932\n",
      "-----------------SAVE:25 epoch----------------\n",
      "epoch : 25/70    time : 175s/7893s\n",
      "TRAIN    loss : 0.02909    f1 : 0.95999\n",
      "Val    loss : 0.14626    f1 : 0.85122\n",
      "epoch : 26/70    time : 175s/7686s\n",
      "TRAIN    loss : 0.02235    f1 : 0.97194\n",
      "Val    loss : 0.20622    f1 : 0.80914\n",
      "epoch : 27/70    time : 175s/7520s\n",
      "TRAIN    loss : 0.02341    f1 : 0.97506\n",
      "Val    loss : 0.16593    f1 : 0.83427\n",
      "epoch : 28/70    time : 174s/7327s\n",
      "TRAIN    loss : 0.01703    f1 : 0.98498\n",
      "Val    loss : 0.15282    f1 : 0.83066\n",
      "epoch : 29/70    time : 174s/7150s\n",
      "TRAIN    loss : 0.01446    f1 : 0.98811\n",
      "Val    loss : 0.13146    f1 : 0.83793\n",
      "epoch : 30/70    time : 174s/6973s\n",
      "TRAIN    loss : 0.01957    f1 : 0.98251\n",
      "Val    loss : 0.15508    f1 : 0.84286\n",
      "-----------------SAVE:31 epoch----------------\n",
      "epoch : 31/70    time : 175s/6825s\n",
      "TRAIN    loss : 0.01901    f1 : 0.98065\n",
      "Val    loss : 0.17430    f1 : 0.85164\n",
      "epoch : 32/70    time : 175s/6640s\n",
      "TRAIN    loss : 0.02223    f1 : 0.97801\n",
      "Val    loss : 0.16516    f1 : 0.83197\n",
      "epoch : 33/70    time : 175s/6465s\n",
      "TRAIN    loss : 0.03365    f1 : 0.96660\n",
      "Val    loss : 0.15472    f1 : 0.79052\n",
      "epoch : 34/70    time : 175s/6282s\n",
      "TRAIN    loss : 0.04220    f1 : 0.96033\n",
      "Val    loss : 0.17238    f1 : 0.82075\n",
      "epoch : 35/70    time : 175s/6111s\n",
      "TRAIN    loss : 0.02033    f1 : 0.98126\n",
      "Val    loss : 0.15607    f1 : 0.82081\n",
      "epoch : 36/70    time : 174s/5932s\n",
      "TRAIN    loss : 0.01554    f1 : 0.98116\n",
      "Val    loss : 0.18523    f1 : 0.80636\n",
      "epoch : 37/70    time : 175s/5760s\n",
      "TRAIN    loss : 0.01655    f1 : 0.98003\n",
      "Val    loss : 0.18773    f1 : 0.78436\n",
      "epoch : 38/70    time : 174s/5583s\n",
      "TRAIN    loss : 0.04347    f1 : 0.96642\n",
      "Val    loss : 0.15659    f1 : 0.79638\n",
      "epoch : 39/70    time : 174s/5403s\n",
      "TRAIN    loss : 0.02357    f1 : 0.97379\n",
      "Val    loss : 0.12725    f1 : 0.82548\n",
      "-----------------SAVE:40 epoch----------------\n",
      "epoch : 40/70    time : 175s/5248s\n",
      "TRAIN    loss : 0.01394    f1 : 0.97952\n",
      "Val    loss : 0.13930    f1 : 0.86236\n",
      "epoch : 41/70    time : 175s/5063s\n",
      "TRAIN    loss : 0.00992    f1 : 0.99013\n",
      "Val    loss : 0.12883    f1 : 0.85483\n",
      "epoch : 42/70    time : 175s/4888s\n",
      "TRAIN    loss : 0.00832    f1 : 0.99315\n",
      "Val    loss : 0.15532    f1 : 0.83352\n",
      "epoch : 43/70    time : 174s/4711s\n",
      "TRAIN    loss : 0.02300    f1 : 0.98083\n",
      "Val    loss : 0.20336    f1 : 0.80622\n",
      "epoch : 44/70    time : 174s/4536s\n",
      "TRAIN    loss : 0.01743    f1 : 0.98391\n",
      "Val    loss : 0.14805    f1 : 0.84195\n",
      "epoch : 45/70    time : 174s/4355s\n",
      "TRAIN    loss : 0.00871    f1 : 0.99209\n",
      "Val    loss : 0.15961    f1 : 0.83704\n",
      "epoch : 46/70    time : 175s/4192s\n",
      "TRAIN    loss : 0.00684    f1 : 0.99426\n",
      "Val    loss : 0.14692    f1 : 0.82741\n",
      "epoch : 47/70    time : 175s/4018s\n",
      "TRAIN    loss : 0.00738    f1 : 0.99262\n",
      "Val    loss : 0.13636    f1 : 0.82563\n",
      "epoch : 48/70    time : 175s/3845s\n",
      "TRAIN    loss : 0.01102    f1 : 0.99054\n",
      "Val    loss : 0.16914    f1 : 0.84068\n",
      "epoch : 49/70    time : 175s/3677s\n",
      "TRAIN    loss : 0.01110    f1 : 0.99349\n",
      "Val    loss : 0.17058    f1 : 0.84196\n",
      "epoch : 50/70    time : 176s/3519s\n",
      "TRAIN    loss : 0.01152    f1 : 0.98861\n",
      "Val    loss : 0.16282    f1 : 0.84350\n",
      "epoch : 51/70    time : 175s/3318s\n",
      "TRAIN    loss : 0.02080    f1 : 0.98252\n",
      "Val    loss : 0.15870    f1 : 0.84668\n",
      "epoch : 52/70    time : 175s/3149s\n",
      "TRAIN    loss : 0.01340    f1 : 0.99140\n",
      "Val    loss : 0.14533    f1 : 0.84334\n",
      "epoch : 53/70    time : 175s/2978s\n",
      "TRAIN    loss : 0.00537    f1 : 0.99756\n",
      "Val    loss : 0.14222    f1 : 0.84041\n",
      "epoch : 54/70    time : 176s/2822s\n",
      "TRAIN    loss : 0.01843    f1 : 0.98596\n",
      "Val    loss : 0.23747    f1 : 0.79748\n",
      "epoch : 55/70    time : 175s/2627s\n",
      "TRAIN    loss : 0.02707    f1 : 0.97346\n",
      "Val    loss : 0.22874    f1 : 0.82101\n",
      "epoch : 56/70    time : 174s/2443s\n",
      "TRAIN    loss : 0.02654    f1 : 0.97119\n",
      "Val    loss : 0.21557    f1 : 0.80980\n",
      "epoch : 57/70    time : 175s/2272s\n",
      "TRAIN    loss : 0.00960    f1 : 0.98432\n",
      "Val    loss : 0.23088    f1 : 0.82921\n",
      "epoch : 58/70    time : 175s/2100s\n",
      "TRAIN    loss : 0.01791    f1 : 0.98485\n",
      "Val    loss : 0.16883    f1 : 0.82700\n",
      "epoch : 59/70    time : 175s/1923s\n",
      "TRAIN    loss : 0.02386    f1 : 0.97294\n",
      "Val    loss : 0.15406    f1 : 0.84314\n",
      "epoch : 60/70    time : 175s/1748s\n",
      "TRAIN    loss : 0.00610    f1 : 0.99057\n",
      "Val    loss : 0.17379    f1 : 0.85616\n",
      "----------fold_4 start!----------\n",
      "-----------------SAVE:1 epoch----------------\n",
      "epoch : 1/70    time : 175s/12045s\n",
      "TRAIN    loss : 1.15400    f1 : 0.18474\n",
      "Val    loss : 0.61343    f1 : 0.28594\n",
      "-----------------SAVE:2 epoch----------------\n",
      "epoch : 2/70    time : 175s/11928s\n",
      "TRAIN    loss : 0.54138    f1 : 0.34982\n",
      "Val    loss : 0.55635    f1 : 0.40953\n",
      "-----------------SAVE:3 epoch----------------\n",
      "epoch : 3/70    time : 176s/11796s\n",
      "TRAIN    loss : 0.39818    f1 : 0.49186\n",
      "Val    loss : 0.38503    f1 : 0.50141\n",
      "-----------------SAVE:4 epoch----------------\n",
      "epoch : 4/70    time : 175s/11547s\n",
      "TRAIN    loss : 0.27933    f1 : 0.62740\n",
      "Val    loss : 0.27355    f1 : 0.59893\n",
      "-----------------SAVE:5 epoch----------------\n",
      "epoch : 5/70    time : 175s/11388s\n",
      "TRAIN    loss : 0.21961    f1 : 0.70027\n",
      "Val    loss : 0.21392    f1 : 0.68220\n",
      "epoch : 6/70    time : 174s/11167s\n",
      "TRAIN    loss : 0.16203    f1 : 0.79376\n",
      "Val    loss : 0.20278    f1 : 0.65778\n",
      "-----------------SAVE:7 epoch----------------\n",
      "epoch : 7/70    time : 175s/11042s\n",
      "TRAIN    loss : 0.11161    f1 : 0.85793\n",
      "Val    loss : 0.20825    f1 : 0.74535\n",
      "epoch : 8/70    time : 175s/10841s\n",
      "TRAIN    loss : 0.09331    f1 : 0.89220\n",
      "Val    loss : 0.21548    f1 : 0.71645\n",
      "epoch : 9/70    time : 175s/10652s\n",
      "TRAIN    loss : 0.08564    f1 : 0.91006\n",
      "Val    loss : 0.18211    f1 : 0.73938\n",
      "-----------------SAVE:10 epoch----------------\n",
      "epoch : 10/70    time : 175s/10478s\n",
      "TRAIN    loss : 0.06424    f1 : 0.91923\n",
      "Val    loss : 0.19851    f1 : 0.75308\n",
      "epoch : 11/70    time : 174s/10292s\n",
      "TRAIN    loss : 0.05951    f1 : 0.94816\n",
      "Val    loss : 0.19035    f1 : 0.74311\n",
      "-----------------SAVE:12 epoch----------------\n",
      "epoch : 12/70    time : 175s/10162s\n",
      "TRAIN    loss : 0.04688    f1 : 0.95263\n",
      "Val    loss : 0.17968    f1 : 0.76844\n",
      "-----------------SAVE:13 epoch----------------\n",
      "epoch : 13/70    time : 175s/9995s\n",
      "TRAIN    loss : 0.04462    f1 : 0.95529\n",
      "Val    loss : 0.20827    f1 : 0.77195\n",
      "epoch : 14/70    time : 174s/9769s\n",
      "TRAIN    loss : 0.04540    f1 : 0.94248\n",
      "Val    loss : 0.19650    f1 : 0.76594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000017?line=37'>38</a>\u001b[0m train_y\u001b[39m=\u001b[39m[]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000017?line=38'>39</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000017?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m (train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000017?line=40'>41</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000017?line=41'>42</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(batch[\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb Cell 11'\u001b[0m in \u001b[0;36mCustom_dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000012?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000012?line=10'>11</a>\u001b[0m   train_transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000012?line=11'>12</a>\u001b[0m         transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000012?line=12'>13</a>\u001b[0m         transforms\u001b[39m.\u001b[39mNormalize(mean \u001b[39m=\u001b[39m [\u001b[39m0.433038\u001b[39m, \u001b[39m0.403458\u001b[39m, \u001b[39m0.394151\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000012?line=15'>16</a>\u001b[0m         \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000012?line=16'>17</a>\u001b[0m     ])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000012?line=17'>18</a>\u001b[0m   img \u001b[39m=\u001b[39m train_transform(img)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000012?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000012?line=19'>20</a>\u001b[0m   test_transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000012?line=20'>21</a>\u001b[0m         transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000012?line=21'>22</a>\u001b[0m         transforms\u001b[39m.\u001b[39mNormalize(mean \u001b[39m=\u001b[39m [\u001b[39m0.418256\u001b[39m, \u001b[39m0.393101\u001b[39m, \u001b[39m0.386632\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000012?line=22'>23</a>\u001b[0m                              std \u001b[39m=\u001b[39m [\u001b[39m0.195055\u001b[39m, \u001b[39m0.190053\u001b[39m, \u001b[39m0.185323\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fds/Dev/Python/pytorch/dacon/anomaly_detection_vision.ipynb#ch0000012?line=23'>24</a>\u001b[0m     ])\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=92'>93</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=93'>94</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=94'>95</a>\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torchvision/transforms/transforms.py:1532\u001b[0m, in \u001b[0;36mRandomAffine.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=1527'>1528</a>\u001b[0m img_size \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mget_image_size(img)\n\u001b[1;32m   <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=1529'>1530</a>\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdegrees, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranslate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshear, img_size)\n\u001b[0;32m-> <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=1531'>1532</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49maffine(img, \u001b[39m*\u001b[39;49mret, interpolation\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, fill\u001b[39m=\u001b[39;49mfill, center\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcenter)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torchvision/transforms/functional.py:1210\u001b[0m, in \u001b[0;36maffine\u001b[0;34m(img, angle, translate, scale, shear, interpolation, fill, resample, fillcolor, center)\u001b[0m\n\u001b[1;32m   <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torchvision/transforms/functional.py?line=1207'>1208</a>\u001b[0m translate_f \u001b[39m=\u001b[39m [\u001b[39m1.0\u001b[39m \u001b[39m*\u001b[39m t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m translate]\n\u001b[1;32m   <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torchvision/transforms/functional.py?line=1208'>1209</a>\u001b[0m matrix \u001b[39m=\u001b[39m _get_inverse_affine_matrix(center_f, angle, translate_f, scale, shear)\n\u001b[0;32m-> <a href='file:///home/fds/miniconda3/envs/py310_pytorch/lib/python3.10/site-packages/torchvision/transforms/functional.py?line=1209'>1210</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49maffine(img, matrix\u001b[39m=\u001b[39;49mmatrix, interpolation\u001b[39m=\u001b[39;49minterpolation\u001b[39m.\u001b[39;49mvalue, fill\u001b[39m=\u001b[39;49mfill)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 5, random_state = 2022,shuffle=True)\n",
    "batch_size = 32\n",
    "epochs = 70\n",
    "pred_ensemble = []\n",
    "\n",
    "\n",
    "for idx, (train_idx, val_idx) in enumerate(cv.split(train_imgs, np.array(train_labels))):\n",
    "  print(\"----------fold_{} start!----------\".format(idx))\n",
    "  t_imgs, val_imgs = train_imgs[train_idx],  train_imgs[val_idx]\n",
    "  t_labels, val_labels = np.array(train_labels)[train_idx], np.array(train_labels)[val_idx]\n",
    "\n",
    "  # Train\n",
    "  train_dataset = Custom_dataset(np.array(t_imgs), np.array(t_labels), mode='train')\n",
    "  train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "  # Val\n",
    "  val_dataset = Custom_dataset(np.array(val_imgs), np.array(val_labels), mode='test')\n",
    "  val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  best=0\n",
    "\n",
    "  model = Network().to(device)\n",
    "\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay = 2e-2)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  scaler = torch.cuda.amp.GradScaler()  \n",
    "\n",
    "  best_f1 = 0\n",
    "  early_stopping = 0\n",
    "  for epoch in range(epochs):\n",
    "    start=time.time()\n",
    "    train_loss = 0\n",
    "    train_pred=[]\n",
    "    train_y=[]\n",
    "    model.train()\n",
    "    for batch in (train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x = torch.tensor(batch[0], dtype=torch.float32, device=device)\n",
    "        y = torch.tensor(batch[1], dtype=torch.long, device=device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss += loss.item()/len(train_loader)\n",
    "        train_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "        train_y += y.detach().cpu().numpy().tolist()\n",
    "    train_f1 = score_function(train_y, train_pred)\n",
    "    state_dict= model.state_dict()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      val_loss = 0 \n",
    "      val_pred = []\n",
    "      val_y = []\n",
    "      \n",
    "\n",
    "      for batch in (val_loader):\n",
    "        x_val = torch.tensor(batch[0], dtype = torch.float32, device = device)\n",
    "        y_val = torch.tensor(batch[1], dtype=torch.long, device=device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred_val = model(x_val)\n",
    "        loss_val = criterion(pred_val, y_val)\n",
    "\n",
    "        val_loss += loss_val.item()/len(val_loader)\n",
    "        val_pred += pred_val.argmax(1).detach().cpu().numpy().tolist()\n",
    "        val_y += y_val.detach().cpu().numpy().tolist()\n",
    "      val_f1 = score_function(val_y, val_pred)\n",
    "\n",
    "      if val_f1 > best_f1:\n",
    "        best_epoch = epoch\n",
    "        best_loss = val_loss\n",
    "        best_f1 = val_f1\n",
    "        early_stopping = 0\n",
    "\n",
    "        torch.save({'epoch':epoch,\n",
    "                    'state_dict':state_dict,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scaler': scaler.state_dict(),\n",
    "             }, path +'best_model_{}.pth'.format(idx))\n",
    "        print('-----------------SAVE:{} epoch----------------'.format(best_epoch+1))\n",
    "      else:\n",
    "          early_stopping += 1\n",
    "\n",
    "            # Early Stopping\n",
    "      if early_stopping == 20:\n",
    "        TIME = time.time() - start\n",
    "        print(f'epoch : {epoch+1}/{epochs}    time : {TIME:.0f}s/{TIME*(epochs-epoch-1):.0f}s')\n",
    "        print(f'TRAIN    loss : {train_loss:.5f}    f1 : {train_f1:.5f}')\n",
    "        print(f'Val    loss : {val_loss:.5f}    f1 : {val_f1:.5f}')\n",
    "        break\n",
    "\n",
    "    TIME = time.time() - start\n",
    "    print(f'epoch : {epoch+1}/{epochs}    time : {TIME:.0f}s/{TIME*(epochs-epoch-1):.0f}s')\n",
    "    print(f'TRAIN    loss : {train_loss:.5f}    f1 : {train_f1:.5f}')\n",
    "    print(f'Val    loss : {val_loss:.5f}    f1 : {val_f1:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ensemble = []\n",
    "batch_size = 32\n",
    "# Test\n",
    "test_dataset = Custom_dataset(np.array(test_imgs), np.array([\"tmp\"]*len(test_imgs)), mode='test')\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "for i in range(5):\n",
    "  model_test = Network(mode = 'test').to(device)\n",
    "  model_test.load_state_dict(torch.load((path+'best_model_{}.pth'.format(i)))['state_dict'])\n",
    "  model_test.eval()\n",
    "  pred_prob = []\n",
    "  with torch.no_grad():\n",
    "      for batch in (test_loader):\n",
    "          x = torch.tensor(batch[0], dtype = torch.float32, device = device)\n",
    "          with torch.cuda.amp.autocast():\n",
    "              pred = model_test(x)\n",
    "              pred_prob.extend(pred.detach().cpu().numpy())\n",
    "      pred_ensemble.append(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (np.array(pred_ensemble[0])+ np.array(pred_ensemble[1])+ np.array(pred_ensemble[3]) + np.array(pred_ensemble[4]) )/4\n",
    "f_pred = np.array(pred).argmax(1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_decoder = {val:key for key, val in label_unique.items()}\n",
    "\n",
    "f_result = [label_decoder[result] for result in f_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tile-glue_strip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>grid-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>transistor-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tile-gray_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tile-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>2149</td>\n",
       "      <td>tile-gray_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>2150</td>\n",
       "      <td>screw-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>2151</td>\n",
       "      <td>grid-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>2152</td>\n",
       "      <td>cable-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>2153</td>\n",
       "      <td>zipper-good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2154 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index             label\n",
       "0         0   tile-glue_strip\n",
       "1         1         grid-good\n",
       "2         2   transistor-good\n",
       "3         3  tile-gray_stroke\n",
       "4         4         tile-good\n",
       "...     ...               ...\n",
       "2149   2149  tile-gray_stroke\n",
       "2150   2150        screw-good\n",
       "2151   2151         grid-good\n",
       "2152   2152        cable-good\n",
       "2153   2153       zipper-good\n",
       "\n",
       "[2154 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(path + \"open/sample_submission.csv\")\n",
    "\n",
    "submission[\"label\"] = f_result\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(path + \"b3_norm_epoch70_4_2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "# Train\n",
    "train_dataset = Custom_dataset(np.array(train_imgs), np.array(train_labels), mode='train')\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Test\n",
    "test_dataset = Custom_dataset(np.array(test_imgs), np.array([\"tmp\"]*len(test_imgs)), mode='test')\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/30    time : 207s/5989s\n",
      "TRAIN    loss : 1.59985    f1 : 0.14687\n",
      "epoch : 2/30    time : 206s/5760s\n",
      "TRAIN    loss : 0.74674    f1 : 0.20972\n",
      "epoch : 3/30    time : 203s/5490s\n",
      "TRAIN    loss : 0.57278    f1 : 0.29085\n",
      "epoch : 4/30    time : 202s/5253s\n",
      "TRAIN    loss : 0.45667    f1 : 0.43380\n",
      "epoch : 5/30    time : 203s/5063s\n",
      "TRAIN    loss : 0.37197    f1 : 0.51530\n",
      "epoch : 6/30    time : 202s/4852s\n",
      "TRAIN    loss : 0.32386    f1 : 0.58196\n",
      "epoch : 7/30    time : 202s/4656s\n",
      "TRAIN    loss : 0.27391    f1 : 0.62976\n",
      "epoch : 8/30    time : 203s/4457s\n",
      "TRAIN    loss : 0.23218    f1 : 0.69038\n",
      "epoch : 9/30    time : 203s/4256s\n",
      "TRAIN    loss : 0.19671    f1 : 0.73789\n",
      "epoch : 10/30    time : 203s/4051s\n",
      "TRAIN    loss : 0.17183    f1 : 0.78702\n",
      "epoch : 11/30    time : 203s/3852s\n",
      "TRAIN    loss : 0.14455    f1 : 0.83397\n",
      "epoch : 12/30    time : 202s/3637s\n",
      "TRAIN    loss : 0.13238    f1 : 0.85739\n",
      "epoch : 13/30    time : 202s/3440s\n",
      "TRAIN    loss : 0.11751    f1 : 0.87701\n",
      "epoch : 14/30    time : 202s/3239s\n",
      "TRAIN    loss : 0.09610    f1 : 0.90838\n",
      "epoch : 15/30    time : 203s/3039s\n",
      "TRAIN    loss : 0.09012    f1 : 0.90476\n",
      "epoch : 16/30    time : 204s/2850s\n",
      "TRAIN    loss : 0.07248    f1 : 0.90537\n",
      "epoch : 17/30    time : 202s/2629s\n",
      "TRAIN    loss : 0.07336    f1 : 0.92398\n",
      "epoch : 18/30    time : 202s/2426s\n",
      "TRAIN    loss : 0.06314    f1 : 0.92670\n",
      "epoch : 19/30    time : 203s/2229s\n",
      "TRAIN    loss : 0.05832    f1 : 0.94195\n",
      "epoch : 20/30    time : 202s/2020s\n",
      "TRAIN    loss : 0.04590    f1 : 0.95435\n",
      "epoch : 21/30    time : 202s/1819s\n",
      "TRAIN    loss : 0.04351    f1 : 0.94326\n",
      "epoch : 22/30    time : 202s/1618s\n",
      "TRAIN    loss : 0.04392    f1 : 0.96209\n",
      "epoch : 23/30    time : 202s/1415s\n",
      "TRAIN    loss : 0.04524    f1 : 0.95246\n",
      "epoch : 24/30    time : 202s/1213s\n",
      "TRAIN    loss : 0.03550    f1 : 0.96764\n",
      "epoch : 25/30    time : 202s/1011s\n",
      "TRAIN    loss : 0.03061    f1 : 0.96724\n",
      "epoch : 26/30    time : 202s/809s\n",
      "TRAIN    loss : 0.03330    f1 : 0.96959\n",
      "epoch : 27/30    time : 202s/606s\n",
      "TRAIN    loss : 0.02699    f1 : 0.96650\n",
      "epoch : 28/30    time : 203s/405s\n",
      "TRAIN    loss : 0.02321    f1 : 0.97986\n",
      "epoch : 29/30    time : 203s/203s\n",
      "TRAIN    loss : 0.03013    f1 : 0.97488\n",
      "epoch : 30/30    time : 202s/0s\n",
      "TRAIN    loss : 0.02350    f1 : 0.97472\n"
     ]
    }
   ],
   "source": [
    "def score_function(real, pred):\n",
    "    score = f1_score(real, pred, average=\"macro\")\n",
    "    return score\n",
    "\n",
    "model = Network().to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay = 1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = torch.cuda.amp.GradScaler() \n",
    "\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "best=0\n",
    "for epoch in range(epochs):\n",
    "    start=time.time()\n",
    "    train_loss = 0\n",
    "    train_pred=[]\n",
    "    train_y=[]\n",
    "    model.train()\n",
    "    for batch in (train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x = torch.tensor(batch[0], dtype=torch.float32, device=device)\n",
    "        y = torch.tensor(batch[1], dtype=torch.long, device=device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss += loss.item()/len(train_loader)\n",
    "        train_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "        train_y += y.detach().cpu().numpy().tolist()\n",
    "        \n",
    "    \n",
    "    train_f1 = score_function(train_y, train_pred)\n",
    "\n",
    "    TIME = time.time() - start\n",
    "    print(f'epoch : {epoch+1}/{epochs}    time : {TIME:.0f}s/{TIME*(epochs-epoch-1):.0f}s')\n",
    "    print(f'TRAIN    loss : {train_loss:.5f}    f1 : {train_f1:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "f_pred = []\n",
    "pred_prob = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in (test_loader):\n",
    "        x = torch.tensor(batch[0], dtype = torch.float32, device = device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = model(x)\n",
    "            pred_prob.extend(pred.detach().cpu().numpy())\n",
    "        f_pred.extend(pred.argmax(1).detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_decoder = {val:key for key, val in label_unique.items()}\n",
    "\n",
    "f_result = [label_decoder[result] for result in f_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tile-glue_strip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>grid-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>transistor-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tile-gray_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tile-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>2149</td>\n",
       "      <td>tile-gray_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>2150</td>\n",
       "      <td>screw-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>2151</td>\n",
       "      <td>grid-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>2152</td>\n",
       "      <td>cable-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>2153</td>\n",
       "      <td>zipper-good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2154 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index             label\n",
       "0         0   tile-glue_strip\n",
       "1         1         grid-good\n",
       "2         2   transistor-good\n",
       "3         3  tile-gray_stroke\n",
       "4         4         tile-good\n",
       "...     ...               ...\n",
       "2149   2149  tile-gray_stroke\n",
       "2150   2150        screw-good\n",
       "2151   2151         grid-good\n",
       "2152   2152        cable-good\n",
       "2153   2153       zipper-good\n",
       "\n",
       "[2154 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(path + \"open/sample_submission.csv\")\n",
    "\n",
    "submission[\"label\"] = f_result\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(path + \" b3_norm.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEl training.....gogo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1c3adb10a68128a1a8313746601dc1ecff251b3157a7a745306942cc936ba57"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
